# R1-RAG: Learning to Plan in Retrieval with GRPO-Optimized Thinking 

<div align="center">

**é€šè¿‡ GRPO ä¼˜åŒ–çš„æ£€ç´¢å¢å¼ºç”Ÿæˆä¸é«˜æ•ˆæ€è€ƒå­¦ä¹ è§„åˆ’**

</div>

## ğŸ“– æ¦‚è¿°

R1-RAG æ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼ºå¤šè·³é—®ç­”ä¸­çš„**è§„åˆ’æ¨ç†**èƒ½åŠ›ã€‚ä¸ä¼ ç»Ÿçš„è¢«åŠ¨æ‰§è¡Œæ£€ç´¢çš„RAGç³»ç»Ÿä¸åŒï¼ŒR1-RAG æ•™å¯¼è¯­è¨€æ¨¡å‹ï¼š

1. **è§„åˆ’æ¨ç†**: å°†å¤æ‚é—®é¢˜åˆ†è§£ä¸ºç»“æ„åŒ–çš„å­ç›®æ ‡ï¼ˆDAGï¼‰
2. **å¯é æ‰§è¡Œ**: æ‰§è¡Œåè°ƒçš„æ£€ç´¢å’Œæ¨ç†
3. **ä»è¿‡ç¨‹ä¸­å­¦ä¹ **: åœ¨ä¸­é—´æ­¥éª¤ä¸Šä½¿ç”¨å¯†é›†ç›‘ç£

### æ ¸å¿ƒåˆ›æ–°

- **åŸºäºDAGçš„è§„åˆ’ç»“æ„**: å­é—®é¢˜ä¹‹é—´çš„æ˜¾å¼ä¾èµ–å»ºæ¨¡
- **åŒé‡å¥–åŠ±æœºåˆ¶**: 
  - é€šè¿‡å›¾ç¼–è¾‘è·ç¦»ï¼ˆGEDï¼‰çš„ç»“æ„å¥–åŠ±
  - é€šè¿‡E5åµŒå…¥ç›¸ä¼¼åº¦çš„è¯­ä¹‰å¥–åŠ±
- **æ¸è¿›å¼æƒé‡é€€ç«**: ä»è¿‡ç¨‹å…³æ³¨åˆ°ç»“æœå…³æ³¨çš„å¹³æ»‘è¿‡æ¸¡
- **GPT-4oæ ‡æ³¨æµæ°´çº¿**: è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡çš„é»„é‡‘è§„åˆ’

## ğŸ—ï¸ Architecture

```
Question â†’ [Planning DAG] â†’ [Sub-Goal Execution] â†’ Answer
              â†“                    â†“
         <plan>               <subPlan>
         Q1 â†’ Q2 â†’ Q3         search â†’ info â†’ subAnswer
              â†“
         [GRPO Training]
              â†“
         R = Î±(t)Â·R_process + R_outcome
```

### Reward Design

```python
R_total = Î±(t) * R_process + R_outcome

where:
  R_process = w_f * format_score      # Format compliance
            + w_s * semantic_score    # E5 embedding similarity  
            + w_g * structure_score   # Graph Edit Distance
            + w_step * step_score     # Sub-goal F1 completion
            
  R_outcome = exact_match(pred, gold)  # Final answer correctness
  
  Î±(t) = 1 / (1 + exp((t - 0.9T) / 10)) # Progressive annealing
```

## ğŸ“¦ Installation

```bash
# Clone repository
git clone https://github.com/blues-kun/R1-RAG.git
cd R1-RAG

# Create environment
conda create -n r1rag python=3.12
conda activate r1rag

# Install PyTorch (adjust for your CUDA version)
pip install torch==2.4.0 --index-url https://download.pytorch.org/whl/cu121

# Install dependencies
pip install -r requirements.txt

# Install R1-RAG
pip install -e .
```

### Optional: Retriever Environment

```bash
# For local retrieval server
conda create -n retriever python=3.12
conda activate retriever

conda install pytorch==2.4.0 pytorch-cuda=12.1 -c pytorch -c nvidia
pip install transformers datasets pyserini
conda install -c pytorch -c nvidia faiss-gpu=1.8.0
pip install uvicorn fastapi
```

## ğŸš€ Quick Start

### 1. Prepare Data with Golden Plans

```bash
# Option A: Generate new annotations with GPT-4o
python scripts/prepare_data.py \
    --dataset hotpotqa \
    --generate_annotations \
    --api_key YOUR_OPENAI_KEY \
    --output_dir data/r1_rag

# Option B: Use pre-annotated data
python scripts/prepare_data.py \
    --input_file path/to/annotated_data.jsonl \
    --output_dir data/r1_rag
```

### 2. Start Retrieval Server

```bash
# Download index and corpus
python scripts/download_index.py --save_path data/retriever

# Launch server
conda activate retriever
python -m r1_rag.retriever.server \
    --index_path data/retriever/e5_Flat.index \
    --corpus_path data/retriever/wiki-18.jsonl \
    --port 8000
```

### 3. Train with GRPO

```bash
conda activate r1rag
bash scripts/train_grpo.sh
```

## ğŸ“Š Data Format

> **æ³¨æ„**: æœ¬é¡¹ç›®çš„è®­ç»ƒæ•°æ®ä½¿ç”¨**è‹±æ–‡æ•°æ®é›†**ï¼ˆHotpotQA, 2WikiMultihopQA, Musiqueç­‰ï¼‰ã€‚ä»¥ä¸‹ç¤ºä¾‹ä½¿ç”¨ä¸­æ–‡ä»…æ˜¯ä¸ºäº†ä¾¿äºç†è§£æ•°æ®æ ¼å¼å’Œæ¨¡å‹è¾“å‡ºç»“æ„ã€‚å®é™…è®­ç»ƒæ—¶ï¼Œæ‰€æœ‰é—®é¢˜ã€ç­”æ¡ˆå’Œè§„åˆ’å‡ä¸ºè‹±æ–‡ã€‚

### Training Sample Structure

```python
{
    "question": "ã€Šæ³°å¦å°¼å…‹å·ã€‹çš„å¯¼æ¼”çš„ç¬¬ä¸€éƒ¨ç”µå½±ï¼Ÿ",
    "golden_answers": ["Piranha II: The Spawning", "é£Ÿäººé±¼2"],
    "metadata": {
        "hop": "2hop",
        "plan": {
            "Q1": ["è°æ‰§å¯¼äº†ã€Šæ³°å¦å°¼å…‹å·ã€‹ï¼Ÿ", "<A1>"],
            "Q2": ["<A1>çš„ç¬¬ä¸€éƒ¨ç”µå½±æ˜¯ä»€ä¹ˆï¼Ÿ", "<A2>"]
        },
        "graph": [{
            "Q1": {"answer": "è©¹å§†æ–¯Â·å¡æ¢…éš†"},
            "Q2": {"answer": "é£Ÿäººé±¼2"}
        }]
    }
}
```

### Expected Model Output

> **è¯´æ˜**: ä»¥ä¸‹ä¸ºä¸­æ–‡ç¤ºä¾‹ï¼Œå®é™…è®­ç»ƒå’Œæ¨ç†ä½¿ç”¨è‹±æ–‡æ•°æ®ã€‚

```xml
<think> è¿™ä¸ªé—®é¢˜éœ€è¦åˆ†ä¸¤æ­¥ï¼šé¦–å…ˆæ‰¾åˆ°ã€Šæ³°å¦å°¼å…‹å·ã€‹çš„å¯¼æ¼”ï¼Œç„¶åæŸ¥è¯¢è¯¥å¯¼æ¼”çš„ç¬¬ä¸€éƒ¨ç”µå½±ã€‚ </think>
<plan>
{"Q1": ["è°æ‰§å¯¼äº†ã€Šæ³°å¦å°¼å…‹å·ã€‹ï¼Ÿ", "#1"], "Q2": ["#1çš„ç¬¬ä¸€éƒ¨ç”µå½±æ˜¯ä»€ä¹ˆï¼Ÿ", "#2"]}
</plan>

<subPlan>
    <think> å…ˆæœç´¢ã€Šæ³°å¦å°¼å…‹å·ã€‹çš„å¯¼æ¼”ä¿¡æ¯ã€‚ </think>
    <search> æ³°å¦å°¼å…‹å· å¯¼æ¼” </search>
    <information> ã€Šæ³°å¦å°¼å…‹å·ã€‹æ˜¯1997å¹´ä¸Šæ˜ çš„å²è¯—çº§çˆ±æƒ…ç¾éš¾ç‰‡ï¼Œç”±è©¹å§†æ–¯Â·å¡æ¢…éš†æ‰§å¯¼... </information>
    <think> æ ¹æ®æ£€ç´¢ç»“æœï¼Œå¯¼æ¼”æ˜¯è©¹å§†æ–¯Â·å¡æ¢…éš†ã€‚ </think>
    <subAnswer> #1 = è©¹å§†æ–¯Â·å¡æ¢…éš† </subAnswer>
</subPlan>

<subPlan>
    <think> ç°åœ¨éœ€è¦æŸ¥è¯¢è©¹å§†æ–¯Â·å¡æ¢…éš†çš„å¯¼æ¼”å¤„å¥³ä½œã€‚ </think>
    <search> è©¹å§†æ–¯Â·å¡æ¢…éš† ç¬¬ä¸€éƒ¨ç”µå½± å¯¼æ¼”å¤„å¥³ä½œ </search>
    <information> è©¹å§†æ–¯Â·å¡æ¢…éš†çš„å¯¼æ¼”å¤„å¥³ä½œæ˜¯1982å¹´çš„ã€Šé£Ÿäººé±¼2ï¼šç¹æ®–ã€‹(Piranha II: The Spawning)... </information>
    <think> ä»–çš„ç¬¬ä¸€éƒ¨ç”µå½±æ˜¯ã€Šé£Ÿäººé±¼2ã€‹ã€‚ </think>
    <subAnswer> #2 = é£Ÿäººé±¼2 </subAnswer>
</subPlan>

<think> å·²è·å–æ‰€æœ‰å­é—®é¢˜çš„ç­”æ¡ˆï¼Œå¯ä»¥ç»™å‡ºæœ€ç»ˆç»“æœã€‚ </think>
<answer> é£Ÿäººé±¼2 </answer>
```




## ğŸ—‚ï¸ Project Structure

```
R1_RAG/
â”œâ”€â”€ r1_rag/
â”‚   â”œâ”€â”€ reward/                 # DAG-based reward computation
â”‚   â”‚   â”œâ”€â”€ config.py           # Reward configuration
â”‚   â”‚   â”œâ”€â”€ dag_evaluator.py    # Main reward evaluator
â”‚   â”‚   â”œâ”€â”€ semantic_scorer.py  # E5 embedding similarity
â”‚   â”‚   â””â”€â”€ structure_scorer.py # Graph Edit Distance
â”‚   â”œâ”€â”€ data/                   # Data processing
â”‚   â”‚   â”œâ”€â”€ processor.py        # Dataset processing
â”‚   â”‚   â”œâ”€â”€ gpt4o_annotator.py  # Golden plan generation
â”‚   â”‚   â””â”€â”€ prompts.py          # Prompt templates
â”‚   â””â”€â”€ agent/                  # Generation loop
â”‚       â””â”€â”€ generation_manager.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_grpo.sh           # Training script
â”‚   â””â”€â”€ prepare_data.py         # Data preparation
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ grpo_qwen_3b.yaml       # Training config
â””â”€â”€ requirements.txt



## ğŸ™ Acknowledgements

This project builds upon several excellent open-source works:

- [veRL](https://github.com/volcengine/verl) - RL training framework
- [Search-R1](https://github.com/PeterGriffinJin/Search-R1) - Reasoning-search interleaving
- [sentence-transformers](https://github.com/UKPLab/sentence-transformers) - E5 embeddings
- [NetworkX](https://networkx.org/) - Graph algorithms


